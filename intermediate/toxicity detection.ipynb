{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNoTE7YeWaX7lUaIdkZtoU/"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rcJhd1TqiDcj","outputId":"d5333208-fc83-402a-f51a-8cf0ab590e54","executionInfo":{"status":"ok","timestamp":1748495181680,"user_tz":-330,"elapsed":181311,"user":{"displayName":"Bhuvan","userId":"01798504154681775302"}}},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/3\n","\u001b[1m2736/2736\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 15ms/step - accuracy: 0.9161 - loss: 0.2452 - val_accuracy: 0.9529 - val_loss: 0.1298\n","Epoch 2/3\n","\u001b[1m2736/2736\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 14ms/step - accuracy: 0.9589 - loss: 0.1162 - val_accuracy: 0.9551 - val_loss: 0.1293\n","Epoch 3/3\n","\u001b[1m2736/2736\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 17ms/step - accuracy: 0.9626 - loss: 0.0981 - val_accuracy: 0.9557 - val_loss: 0.1308\n","\u001b[1m684/684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step\n","Accuracy: 0.9556774046150331\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.96      0.99      0.98     19199\n","           1       0.89      0.73      0.80      2686\n","\n","    accuracy                           0.96     21885\n","   macro avg       0.92      0.86      0.89     21885\n","weighted avg       0.95      0.96      0.95     21885\n","\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n","Sample Input: ['you  are so kind']\n","Output: [0]\n"]}],"source":["import pandas as pd\n","import numpy as np\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Dropout, GlobalMaxPooling1D\n","from tensorflow.keras.layers import Embedding\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from tensorflow.keras.utils import to_categorical\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score, classification_report\n","\n","# Load the dataset\n","train_df = pd.read_csv('preprocessed_dataset1.csv', on_bad_lines='skip', engine='python')\n","\n","# Split data into training and validation sets\n","X_train, X_val, y_train, y_val = train_test_split(train_df['comment_text'], train_df['toxic'], test_size=0.2, random_state=42)\n","\n","# Create a tokenizer\n","tokenizer = Tokenizer(num_words=5000)\n","tokenizer.fit_on_texts(X_train)\n","\n","# Convert text to sequences\n","X_train_seq = tokenizer.texts_to_sequences(X_train)\n","X_val_seq = tokenizer.texts_to_sequences(X_val)\n","\n","# Pad sequences\n","max_length = 100\n","X_train_pad = pad_sequences(X_train_seq, maxlen=max_length)\n","X_val_pad = pad_sequences(X_val_seq, maxlen=max_length)\n","\n","# One-hot encode labels\n","y_train_cat = to_categorical(y_train)\n","y_val_cat = to_categorical(y_val)\n","\n","# Build the model\n","model = Sequential()\n","model.add(Embedding(5000, 128, input_length=max_length))\n","model.add(GlobalMaxPooling1D())\n","model.add(Dense(64, activation='relu'))\n","model.add(Dropout(0.5))\n","model.add(Dense(2, activation='softmax'))\n","model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n","\n","# Train the model\n","model.fit(X_train_pad, y_train_cat, epochs=3, batch_size=32, validation_data=(X_val_pad, y_val_cat))\n","\n","# Evaluate the model\n","y_pred = model.predict(X_val_pad)\n","y_pred_class = np.argmax(y_pred, axis=1)\n","y_val_class = np.argmax(y_val_cat, axis=1)\n","print('Accuracy:', accuracy_score(y_val_class, y_pred_class))\n","print('Classification Report:')\n","print(classification_report(y_val_class, y_pred_class))\n","\n","# Sample input and output\n","sample_text = ['you  are so kind']\n","sample_seq = tokenizer.texts_to_sequences(sample_text)\n","sample_pad = pad_sequences(sample_seq, maxlen=max_length)\n","prediction = model.predict(sample_pad)\n","print('Sample Input:', sample_text)\n","print('Output:', np.argmax(prediction, axis=1))"]}]}